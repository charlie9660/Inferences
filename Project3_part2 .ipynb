{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from vecstack import stacking\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "import numpy as np\n",
    "\n",
    "class Project:   \n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "            \n",
    "    def _load_data_(self,filepath):\n",
    "      \n",
    "        #if (~os.path.isfile('train.csv')) | (~os.path.isfile('test.csv')):\n",
    "           # self.data = pd.read_csv(filepath,encoding='ISO-8859-1')\n",
    "           # self.train_test_split()\n",
    "           # self._write_data_()\n",
    "            \n",
    "        self.train_data = pd.read_csv('train.csv')\n",
    "                                #,encoding='ISO-8859-1')\n",
    "        self.test_data = pd.read_csv('test.csv',encoding='ISO-8859-1')\n",
    "        return self.train_data\n",
    "        \n",
    "    def _write_data_(self):\n",
    "        self.train_data.to_csv('train.csv')\n",
    "        self.test_data.to_csv('test.csv')\n",
    "    \n",
    "    def train_test_split(self):\n",
    "        self.train_data, self.test_data = train_test_split(self.data, test_size=0.2, random_state=42)   \n",
    "        \n",
    "    def null_summary(self,df):\n",
    "        print(df.isnull().sum())\n",
    "        all_data_na = (df.isnull().sum() / len(df)) * 100\n",
    "        all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n",
    "        missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "        missing_data.head()\n",
    "        if missing_data.size> 0:\n",
    "            f, ax = plt.subplots(figsize=(15, 12))\n",
    "            plt.xticks(rotation='90')\n",
    "            sns.barplot(x=all_data_na.index, y=all_data_na)\n",
    "            plt.xlabel('Features', fontsize=15)\n",
    "            plt.ylabel('Percent of missing values', fontsize=15)\n",
    "            plt.title('Percent missing data by feature', fontsize=15)\n",
    "            plt.show()\n",
    "            \n",
    "    def get_stats(self,df,variable):\n",
    "        print(\"Skewness: %f\" % df[variable].skew())\n",
    "        print(\"Kurtosis: %f\" % df[variable].kurt())\n",
    "        print(\"Mean: %f\" % df[variable].mean())\n",
    "        print(\"Variance: %f\" % (df[variable].var()))\n",
    "\n",
    "            \n",
    "    def correlation_map(self,df,outcome,k):\n",
    "        corrmat = df.corr()\n",
    "        col_large = corrmat.nlargest(k+1,outcome)[outcome].index\n",
    "        col_small = corrmat.nsmallest(k,outcome)[outcome].index\n",
    "        cols = col_large.union(col_small)\n",
    "        cm = np.corrcoef(corrmat[cols].loc[cols].values.T)\n",
    "        print(corrmat[cols].loc[outcome])\n",
    "        #sns.set(font_scale=1.25)\n",
    "        hm = sns.heatmap(cm, cbar=True, vmax=1,annot=True, fmt='.1f', annot_kws={'size': 8}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "        return cols\n",
    "    \n",
    "    def encode_organization(self,x):\n",
    "            if 'Academy' in x:\n",
    "                return 1\n",
    "            elif 'Instituion' in x:\n",
    "                return 2\n",
    "            elif 'College' in x:\n",
    "                return 3\n",
    "            elif 'University' in x:\n",
    "                return 4\n",
    "            else:\n",
    "                return 0\n",
    "    \n",
    "    def encode_cost(self,df):\n",
    "        prices = []\n",
    "        for i in range(len(df)):\n",
    "            if pd.notnull(df['NPT4_PUB'][i]):\n",
    "                prices.append(df['NPT4_PUB'][i])\n",
    "            elif pd.notnull(df['NPT4_PRIV'][i]):\n",
    "                prices.append(df['NPT4_PRIV'][i])\n",
    "            else:\n",
    "                prices.append(np.NaN) \n",
    "        \n",
    "        df.loc[:,'NET_COST'] = pd.Series(prices, index=df.index)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def encode_25KBinary(self, df):\n",
    "        vals = []\n",
    "        for i in range(len(df)):\n",
    "            if float(df['gt_25k_p6'][i]) >= 0.6:\n",
    "                vals.append(1)\n",
    "            elif float(df['gt_25k_p6'][i]) < 0.6:\n",
    "                vals.append(0)\n",
    "            else:\n",
    "                vals.append(np.NaN)\n",
    "        \n",
    "        df.loc[:,'Binary_25k'] = pd.Series(vals, index=df.index)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def encode_state(self,x):\n",
    "        west = ['WA','MT','OR','ID','WT','CA','NV','UT','CO','AZ','NM']\n",
    "        midwest = ['ND','MN','SD','NE','KS','IA','MO','WI','IL','MI','IN','OH','WY']\n",
    "        northeast = ['NY','PA','NJ','RI','CT','MA','VT','NH','ME']\n",
    "        south = ['TX','OK','AR','LA','MS','AL','GA','TN','KY','WV','DC','MD','VA','DE','NC','SC','FL']\n",
    "        others =['AK','HI','PR','PW','AS','GU','FM','VI']\n",
    "        \n",
    "        if x in others:\n",
    "            return 0\n",
    "        elif x in midwest:\n",
    "            return 1\n",
    "        elif x in south:\n",
    "            return 2\n",
    "        elif x in west:\n",
    "            return 3\n",
    "        elif x in northeast:\n",
    "            return 4\n",
    "        else: return x\n",
    "        \n",
    "    def compute_ci(self,x):\n",
    "        print('tn,p,dof =',sms.DescrStatsW(x).ttest_mean(x.mean()))\n",
    "        print('CI =',sms.DescrStatsW(x).tconfint_mean())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Data Load##########\n",
    "\n",
    "project = Project()\n",
    "project._load_data_('train.csv')\n",
    "df = project.train_data\n",
    "df = df.drop(df.columns[0],axis=1)\n",
    "df = df.apply(lambda x: x.replace('PrivacySuppressed',np.NAN))\n",
    "df = project.encode_cost(df)\n",
    "df = project.encode_25KBinary(df)\n",
    "df['RPY_3YR_RT_SUPP'] = pd.to_numeric(df['RPY_3YR_RT_SUPP'])\n",
    "df['md_earn_wne_p10'] = pd.to_numeric(df['md_earn_wne_p10'])\n",
    "df['GRAD_DEBT_MDN_SUPP'] = pd.to_numeric(df['GRAD_DEBT_MDN_SUPP'])\n",
    "df['GRAD_DEBT_MDN10YR_SUPP'] = pd.to_numeric(df['GRAD_DEBT_MDN10YR_SUPP'])\n",
    "df['C150_4_POOLED_SUPP'] = pd.to_numeric(df['C150_4_POOLED_SUPP'])\n",
    "df['C200_L4_POOLED_SUPP'] = pd.to_numeric(df['C200_L4_POOLED_SUPP'])\n",
    "columns = df.columns\n",
    "treatment_variables = columns.drop(['INSTNM','CITY','STABBR','RPY_3YR_RT_SUPP', 'md_earn_wne_p10', 'gt_25k_p6', 'Binary_25k'])\n",
    "#only keep the variables if less than 20% of the values are missing ( SHOULD I KEEP IT at 20%?)\n",
    "keep_variables = []\n",
    "for i in range(len(treatment_variables)):\n",
    "    x = df[str(treatment_variables[i])].isnull().sum()/float((len(df)))\n",
    "    if x <= 0.2:\n",
    "        keep_variables.append(treatment_variables[i]) \n",
    "target_var = 'RPY_3YR_RT_SUPP'\n",
    "keep_variables.append(target_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[keep_variables]\n",
    "df = df.apply(lambda x: x.fillna(x.median()),axis=0)\n",
    "\n",
    "train_X = df.loc[:, df.columns != target_var]\n",
    "train_Y = df[target_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV,LinearRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from vecstack import stacking\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n",
    "\n",
    "\n",
    "models = [\n",
    "#     ExtraTreesRegressor(random_state = 0, n_jobs = 2, \n",
    "#         n_estimators = 100, max_depth = 3),\n",
    "    \n",
    "#     Pipeline([\n",
    "#       ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\"))),\n",
    "#       ('regression',randomForestRegressor(random_state = 0, n_jobs = 2, \n",
    "#         n_estimators = 100, max_depth = 3))\n",
    "#     ]),\n",
    "    \n",
    "#      make_pipeline(SelectFromModel(ExtraTreesRegressor(random_state = 0, n_jobs = 2, n_estimators = 500, max_depth = 3)),RandomForestRegressor(random_state = 0, n_jobs = 2, \n",
    "#          n_estimators = 100, max_depth = 3)),\n",
    "        \n",
    "#     lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "#                               learning_rate=0.05, n_estimators=2000,\n",
    "#                               max_bin = 60, bagging_fraction = 0.8,\n",
    "#                               bagging_freq = 5, feature_fraction = 0.2319,\n",
    "#                               feature_fraction_seed=9, bagging_seed=9,\n",
    "#                               min_data_in_leaf =6, min_sum_hessian_in_leaf = 11),\n",
    "    \n",
    "    make_pipeline(RobustScaler(), LassoCV(alphas =[0.0005,0.001,0.0015], random_state=1,cv=3)),\n",
    "    make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)),\n",
    "    KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5),\n",
    "\n",
    "    GradientBoostingRegressor(n_estimators=2000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5),\n",
    "    \n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "kfold = 5\n",
    "# cv_results = []\n",
    "# cv_means = []\n",
    "# cv_std = []\n",
    "# for model in models :\n",
    "#     cv_results.append(cross_val_score(model, train_X, y = train_Y, scoring = \"neg_mean_squared_error\", cv = kfold, n_jobs=4))\n",
    "\n",
    "# for cv_result in cv_results:\n",
    "#     cv_means.append(cv_result.mean())\n",
    "#     cv_std.append(cv_result.std())\n",
    "\n",
    "# cv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"Lasso\",\"ElasticNet\",\"GradientBoosting\"]})\n",
    "\n",
    "# g = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\n",
    "# g.set_xlabel(\"Mean Accuracy\")\n",
    "# g = g.set_title(\"Cross validation scores\")\n",
    "\n",
    "\n",
    "# Compute stacking features\n",
    "#S_train, S_test = stacking(models, train_X.values, train_Y.values, test_X.values, \n",
    "#    regression = True, metric = mean_squared_error, n_folds = 20, \n",
    "#    shuffle = True, random_state = 0, verbose = 2)\n",
    "\n",
    "\n",
    "#model = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0001, l1_ratio=.9, random_state=3))\n",
    "\n",
    "#model = RandomForestRegressor(random_state = 0, n_jobs = 2, n_estimators = 1000, max_depth = 3)\n",
    "    \n",
    "# Fit 2-nd level model\n",
    "#model = model.fit(S_train, train_Y.values)\n",
    "\n",
    "# Predict\n",
    "#y_pred = model.predict(S_train)\n",
    "\n",
    "# Final prediction score\n",
    "#print('Final prediction score: [%.8f]' % mean_squared_error(train_Y.values, y_pred))\n",
    "#y_pred = model.predict(S_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:   37.3s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=4)]: Done 540 out of 540 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.014204198042550534"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ExtraTrees \n",
    "ExtC = ExtraTreesRegressor()\n",
    "## Search grid for optimal parameters\n",
    "ex_param_grid = {\"max_depth\": [5],\n",
    "              \"max_features\": [1, 3, 5, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [False],\n",
    "              \"n_estimators\" :[100,200,500]\n",
    "              }\n",
    "\n",
    "\n",
    "gsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"neg_mean_squared_error\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsExtC.fit(train_X,train_Y)\n",
    "\n",
    "ExtC_best = gsExtC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsExtC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed:   41.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.012303051456565317"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFC = RandomForestRegressor()\n",
    "\n",
    "\n",
    "## Search grid for optimal parameters\n",
    "rf_param_grid = {\"max_depth\": [5],\n",
    "              \"max_features\": [1, 3, 5,10],\n",
    "              #\"min_samples_split\": [2, 3, 10],\n",
    "              #\"min_samples_leaf\": [1, 3, 10],\n",
    "              #\"bootstrap\": [False],\n",
    "              \"n_estimators\" :[100,200,500]}\n",
    "\n",
    "\n",
    "gsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold,scoring=\"neg_mean_squared_error\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsRFC.fit(train_X,train_Y)\n",
    "\n",
    "RFC_best = gsRFC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsRFC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=4)]: Done 480 out of 480 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.0096090257071771643"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBC = GradientBoostingRegressor()\n",
    "gb_param_grid = {\n",
    "              'n_estimators' : [100,200,500],\n",
    "              'learning_rate': [0.05,0.02,0.01,0.005],\n",
    "              'max_depth': [4, 8],\n",
    "              'min_samples_leaf': [100,150],\n",
    "              'max_features': [0.3, 0.1] \n",
    "              }\n",
    "\n",
    "gsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold,scoring=\"neg_mean_squared_error\",n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsGBC.fit(train_X,train_Y)\n",
    "\n",
    "GBC_best = gsGBC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsGBC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.016685755888763913"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR = LinearRegression()\n",
    "parameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\n",
    "grid = GridSearchCV(LR,parameters, scoring=\"neg_mean_squared_error\",cv=kfold)\n",
    "grid.fit(train_X,train_Y)\n",
    "LR_best = grid.best_estimator_\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   46.4s\n",
      "[Parallel(n_jobs=4)]: Done  80 out of  80 | elapsed:   59.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.0096013012308369412"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTC = DecisionTreeRegressor()\n",
    "\n",
    "adaDTC = AdaBoostRegressor(DTC, random_state=7)\n",
    "\n",
    "ada_param_grid = {\n",
    "              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "              \"n_estimators\" :[30,50],\n",
    "              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1]}\n",
    "\n",
    "gsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"neg_mean_squared_error\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsadaDTC.fit(train_X,train_Y)\n",
    "\n",
    "ada_best = gsadaDTC.best_estimator_\n",
    "gsadaDTC.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
